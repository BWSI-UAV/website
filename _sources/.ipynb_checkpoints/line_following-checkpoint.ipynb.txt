{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Following\n",
    "\n",
    "In this practical, you will learn how to put together your parameterization of a line detected from the downward camera with a flight controller to accurately follow a line.\n",
    "\n",
    "Key tools:\n",
    "\n",
    "- `opencv`\n",
    "- Closed-loop velocity control\n",
    "- `rosmsg`\n",
    "\n",
    "**SLIDES:** [Lecture Slides](https://docs.google.com/presentation/d/11v5SoH-QA0CIVtZMPHNX1Fvcs9RcCkRLHZL-UDwicCE/edit?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking Down Line Following\n",
    "\n",
    "### Computer Vision (`line_tracker/src/detector.py`)\n",
    "\n",
    "Your computer vision algorithms must be precise and accurate when detecting the LED strip that will feature in the final race. \n",
    "\n",
    "Be sure to test your existing CV algorithms by publishing an Image message on `/line/img` that shows the line being tracked. You will be testing this on a `rosbag` **before you have the opportunity to fly your code**. Notify an instructor once you have the proper image displayed on top of the `rosbag` we have provided you. You will then be permitted to move on to the next stage of development in this task.\n",
    "\n",
    "### Control Systems (`line_tracker/src/tracker.py`)\n",
    "\n",
    "You will be controlling three degrees of freedom in the line following task: $x$, $y$, and $z$\n",
    "\n",
    "Using information about the image that the downward camera is publishing, you will calculate the error between your desired position on the line and your current position. \n",
    "\n",
    "This can be broken down into a few steps:\n",
    "\n",
    "- Find the point representing the center of the image $p_{frame-center}$, and the point representing the center of the line $p_{line-center}$ (in pixel coordinates)\n",
    "- Find the unit vector in the direction of the line $\\vec{r}_{line-unit}$ (using `vx` and `vy`)\n",
    "- Find the point on the line closest to the center of the frame $p_{line-closest-center}$ \n",
    "- Extrapolate a point forward (from $p_{line-closest-center}$) on the line using the unit vector in the direction of the line, making sure that the unit vector is positive $p_{target}$\n",
    "- Calculate the vector from the center of the frame to the extrapolated point (this represents error) $\\vec{r}_{to-target}$\n",
    "- Select safe, appropriate gains for your proportional velocity controller than enable the drone to change its velocity to track a the target on the line. Should its response be aggressive? Consider the pros and cons of aggressive vs non-aggressive control behavior. Your $K_{P_x}$, $K_{P_y}$, $K_{P_ZW}$ should always start below 1.\n",
    "- Calculate $v_x(t)$, $v_y(t)$, and $v_z(t)$ using your pre-determined $K_P$ values.\n",
    "\n",
    "\n",
    "Diagram:\n",
    "\n",
    "![line diagram](https://github.com/BWSI-UAV/website/blob/master/docs/images/line_diagram.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Test Flight\n",
    "\n",
    "Once `detector.py` and `tracker.py` have been completed and pulled onto the drone, run the flight test by following these procedures\n",
    "\n",
    "- Power on remote control\n",
    "- Power on the Intel RTF\n",
    "- Connect to the UAV via QGroundControl\n",
    "- SSH into the UAV (you will need to have at least four terminals SSHed into the UAV). use X-forwarding in order to pass graphics (i.e. stream the camera feed from the drone to your computer). Example replacing `<team-drone-name>` with the name of your drone:\n",
    "\n",
    "    ```\n",
    "    ssh -X uav@<team-drone-name>.beaver.works\n",
    "    ```\n",
    "    \n",
    "- In terminal 1, start distance sensor\n",
    "\n",
    "\t```\n",
    "    sudo systemctl start aero-teraranger.service\n",
    "\t```\n",
    "    \n",
    "- In terminal 1, start ROS\n",
    "\n",
    "\t```\n",
    "\troscore\n",
    "\t```\n",
    "\n",
    "- In terminal 2, start _patched_ optical flow + down-camera streaming. Note: this is different from _standard_ optical flow service that prevents streaming downward-facing camera. Needed for line detection. [See here for more info on setting up](https://github.mit.edu/ma23705/aero_downward_ros/blob/master/README.md)\n",
    "\n",
    "\t```\n",
    "\tcd ~/bwsi-uav/catkin_ws/src/aero-optical-flow/build\n",
    "\tsudo -E ./aero-optical-flow\n",
    "\t```\n",
    "\n",
    "- Without arming drone, switch to `POSITION CONTROL` mode to ensure previous steps worked as expect. If QGroundControl declares that that position control is rejected, use QGroundControl > Widgets > MAVLink Inspector to debug and potentially restart process.\n",
    "\n",
    "- In terminal 3, launch `mavros`, `detector`, and `tracker` using `detect_track.launch`\n",
    "\n",
    "\t```\n",
    "\tcd ~/bwsi-uav/catkin_ws\n",
    "\tsource devel/setup.bash\n",
    "\troslaunch aero_control detect_track.launch\n",
    "\t```\n",
    "    \n",
    "- (OPTIONAL) In terminal 4, visualize the downward camera feed for line detection.\n",
    "\n",
    "    ```\n",
    "    rqt_image_view\n",
    "    ```\n",
    "    \n",
    "    and select `/line/detector_image` from the dropdown menu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "## Test Flight\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**WARNING:** \n",
    "\n",
    "Pilot-in-Command must always be ready to regain control by switching back to `POSCTL` mode. Never take your hands off the remote control!\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "In `POSITION CONTROL` mode \n",
    "\n",
    "1. Arm the quadrotor\n",
    "2. Takeoff\n",
    "3. Position in a safe location in the air\n",
    "4. Switch to `OFFBOARD` mode to run test\n",
    "5. Regain control with `POSITION` mode\n",
    "6. Land quadrotor\n",
    "7. Disarm\n",
    "8. Collect [flight log](flight_log_analysis.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
